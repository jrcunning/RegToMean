---
title: "Regression to the mean and the detection of true relationships between change and initial values"
author: "Ross Cunning"
date: "2025-04-13"
output:
  html_document:
    toc: true
    toc_float: true
    toc_depth: 2
    theme: default  # or "lumen", "flatly", etc.
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, message = FALSE, warning = FALSE, cache = TRUE)
```

```{r libraries, echo = FALSE}
# Load libraries
library(tidyverse)
library(ggpubr)
library(ggpmisc)
library(broom)
```

```{r functions, echo = FALSE}
# Define helper functions

# Function to correct for spurious slopes, from Kelly and Price 2005
rttm.adj <- function(m1, m2){
  raw.growth<-m2-m1
  vart<-var.test(m1,m2,paired = T) ## variances equal? 
  vpv<-vart$p.value # var.test p value
  m1m2cor<-cor.test(m1, m2) # test correlation between m1 and m2 
  rho<-m1m2cor$estimate # correlation coefficient between m1 and m2 
  m1sd<-sd(m1) # m1 sd
  m2sd<-sd(m2) # m2 sd
  m1v<-var(m1) # m1 var
  m2v<-var(m2) # m2 var
  m1m<-mean(m1) # m1 mean
  m2m<-mean(m2) # m2 mean
  pm<-mean(raw.growth)
  rho2<-(2*rho*m1sd*m2sd)/(m1v+m2v) # adjusted correlation coefficient used if variances are equal
  rhof<-ifelse(vpv <= 0.05, rho, rho2) # which rho is used for dstar calculation is based on variance comparison
  dstar<-(rhof*(m1-m1m)-(m2-m2m))*-1 # adjustment values. Multiply by -1 to flip sign because Kelly and Price based on plasticity as m1-m2, not m2-m1 as in most thermal tolerance estimates
  adj.growth <- pm+dstar # corrected plasticity. 
  out<-as.data.frame(cbind(raw.growth, dstar, adj.growth)) 
  return(out)
}



## GGplot var.test
stat_var_test <- function(mapping = NULL, data = NULL,
                          method = var.test,
                          xvar = NULL, yvar = NULL,
                          label.x = NULL, label.y = NULL,
                          position = "identity",
                          na.rm = FALSE, show.legend = NA,
                          inherit.aes = TRUE, ...) {

  layer(
    stat = StatVarTest,
    data = data,
    mapping = mapping,
    geom = "text",
    position = position,
    show.legend = show.legend,
    inherit.aes = inherit.aes,
    params = list(
      method = method,
      xvar = xvar,
      yvar = yvar,
      label.x = label.x,
      label.y = label.y,
      na.rm = na.rm,
      ...
    )
  )
}

StatVarTest <- ggproto("StatVarTest", Stat,
  required_aes = c("x", "y"),
  compute_group = function(data, scales, method, xvar, yvar, label.x, label.y) {
    cat("Number of rows in compute_group:", nrow(data), "\n")
    # Perform the test
    vtest <- method(data$x, data$y)

    # Create label
    label <- paste0("var.ratio = ", signif(vtest$estimate, 3),
                    ",\np = ", signif(vtest$p.value, 3))

    # Determine label position
    data.frame(
      x = label.x %||% mean(range(data$x, na.rm = TRUE)),
      y = label.y %||% max(data$y, na.rm = TRUE),
      label = label
    )
  }
)



# Chiolero plot function
chiolero_plot <- function(df, init_var, final_var, label = NULL) {
  init_sym <- ensym(init_var)
  final_sym <- ensym(final_var)
  init_name <- as_label(init_sym)
  final_name <- as_label(final_sym)

  # Get min and max across both variables
  y_range <- range(c(pull(df, !!init_sym), pull(df, !!final_sym)), na.rm = TRUE)

  ggplot(df, aes(x = 1, xend = 2, y = !!init_sym, yend = !!final_sym)) +
    geom_segment(alpha = 0.1) +
    geom_point(aes(x = 1, y = !!init_sym), size = 1.5, alpha = 0.2) +
    geom_point(aes(x = 2, y = !!final_sym), size = 1.5, alpha = 0.2) +
    scale_y_continuous(limits = y_range) +
    scale_x_continuous(breaks = c(1, 2), labels = c(init_name, final_name),
                       expand = c(0.15, 0.15)) +
    labs(x = "timepoint", y = "value") +
    stat_var_test(
      mapping = aes(x = !!init_sym, y = !!final_sym, group = 1),
      label.x = -Inf,
      label.y = Inf,
      hjust = -0.1, vjust = 1.5
    )
}

```


# Objectives

Use simulated datasets to test conditions under which mathematical coupling, regression to the mean, and statistical adjustments to control for these artifacts could:  

1. correctly recover the true relationship between change and initial values;
2. produce a spurious relationship when there is not one;  
2. fail to recover a true relationship when there is one.  

Simulations have either no relationship between change and initial values ([Scenario 1](#Scenario 1: No relationship between change and initial)), or a true relationship specified by parameter *b* ([Scenario 2](# Scenario 2: True negative relationship between change and initial)), and include three sources of variance:  

1. Variance among individuals in the true value (population variance; *v_pop*)  
2. Variance in within-individual changes over time (individual-level variance; *v_ind*)  
3. Variance due to noise or imprecision in measurement (error variance; *v_err*))       

Parameter space then is explored through a [simulation study](# Simulation study of observed and adjusted correlation reliability) to find which combinations of *b*, *v_ind*, and *v_err* give rise to true or spurious relationships based on observed values or those corrected with the Kelly Price adjustment for regression to the mean.

Following simulations, analyze two coral datasets describing [symbiont changes in *P. damicornis*](# Dataset 1: *P. damicornis* symbiont changes) and [growth in *A. cervicornis*](# Dataset 2: *A. cervicornis* growth) and assess the likelihood of the true relationship given observed values in the context of simulations.

# Scenario 1: No relationship between change and initial

## 1.1: Equal variances 
### v_pop == v_ind == v_err

```{r s1.1, fig.width = 10, fig.height = 5}
# Set number of individuals for simulations
n <- 1000

# Set variances
v_pop <- 1
v_ind <- 1
v_err <- 1

# Simulate data
set.seed(1)
df1 <- tibble(
  log_init_true = rnorm(n, mean = 0, sd = sqrt(v_pop)),
  log_change_true = rnorm(n, mean = 0, sd = sqrt(v_ind)),        # Independent 0 change plus individual response variation
  # Later: Check if FINAL can be simulated to then derive change, and if this is equivalent
  log_final_true = log_init_true + log_change_true,           # Final = initial + change
  e_init = rnorm(n, mean = 0, sd = sqrt(v_err)),          # Random measurement error for initial values
  e_final = rnorm(n, mean = 0, sd = sqrt(v_err)),         # Random measurement error for final values
  log_init_obs = log_init_true + e_init,          # Add random measurement error to initial values
  log_final_obs = log_final_true + e_final,       # Add random measurement error to final values
  log_change_obs = log_final_obs - log_init_obs   # Observed change
)  

# Apply Kelly Price correction to true values and observed values
df1 <- df1 %>% mutate(
  log_change_true_adj = rttm.adj(log_init_true, log_final_true)[, 3],
  log_change_obs_adj = rttm.adj(log_init_obs, log_final_obs)[, 3]
)

# ## Compute plot limit values
max_abs_initfinal <- df1 %>%
  select(log_init_true, log_final_true, log_init_obs, log_final_obs) %>%
  summarise(max_abs = max(abs(c_across(everything())), na.rm = TRUE)) %>%
  pull(max_abs)
max_abs_initchange <- df1 %>%
  select(log_init_true, log_change_true, log_init_obs, log_change_obs) %>%
  summarise(max_abs = max(abs(c_across(everything())), na.rm = TRUE)) %>%
  pull(max_abs)

# Plot 1: TRUE Chiolero style initial and final values, timepoints on x-axis
p1 <- chiolero_plot(df1, log_init_true, log_final_true)

# Plot 2: TRUE final vs. initial scatterplot
p2 <- ggplot(df1, aes(x = log_init_true, y = log_final_true)) +
  geom_point(alpha = 0.2) +
  geom_abline(slope = 1, intercept = 0, linetype = "dashed") +
  stat_cor(label.x = -Inf, label.y = Inf, hjust = -0.1, vjust = 1.5) +
  geom_smooth(method = "lm", se = FALSE) +
  coord_fixed(xlim = c(-max_abs_initfinal, max_abs_initfinal), 
              ylim = c(-max_abs_initfinal, max_abs_initfinal))

# Plot 3: TRUE change vs. initial scatterplot
p3 <- ggplot(df1, aes(x = log_init_true, y = log_change_true)) +
  geom_point(alpha = 0.2) +
  geom_abline(slope = 0, intercept = 0, linetype = "dashed") +
  stat_cor(label.x = -Inf, label.y = Inf, hjust = -0.1, vjust = 1.5) +
  geom_smooth(method = "lm", se = FALSE) +
  coord_fixed(xlim = c(-max_abs_initchange, max_abs_initchange), 
              ylim = c(-max_abs_initchange, max_abs_initchange))

# Plot 4: ADJUSTED TRUE change vs. initial scatterplot
p4 <- p3 + aes(x = log_init_true, y = log_change_true_adj)

# Plot 5: OBSERVED Chiolero style
p5 <- chiolero_plot(df1, log_init_obs, log_final_obs)

# Plot 6: OBSERVED final vs. initial scatterplot
p6 <- p2 + aes(x = log_init_obs, y = log_final_obs)

# Plot 7: OBSERVED change vs. initial scatterplot
p7 <- p3 + aes(x = log_init_obs, y = log_change_obs)

# Plot 8: ADJUSTED OBSERVED change vs. initial scatterplot
p8 <- p4 + aes(x = log_init_obs, y = log_change_obs_adj)

plots1 <- list(p1, p2, p3, p4, p5, p6, p7, p8)

plots1panel <- ggpubr::ggarrange(plotlist = plots1, nrow = 2, ncol = 4, labels = "AUTO")

plots1panel
```

* **A** confirms positive (but imperfect) correlation between true initial and final values (biologically realistic)
* **B** confirms no relationship between true change and true initial (because simulated independently)
* **C** confirms Kelly and Price correction on true values yields false positive correlation between change and initial (because initial and final imperfectly correlated due to individual variation)
* **D** confirms positive (but more imperfect, compare to **A**) correlation between initial and final values observed with measurement error
* **E** confirms spurious negative relationship between observed change and initial is due to measurement error (compare to **B**)
* **F** confirms Kelly and Price correction on observed values yields false positive correlation, but weak (compare to **C**) (this becomes statistically significant at higher n)


## 1.2: Variance in changes mostly due to measurement error
### v_pop == v_err >>> v_ind

```{r s1.2, fig.width = 10, fig.height = 5}
# Set variances
v_pop <- 1
v_ind <- 0.001
v_err <- 1

# Simulate data
set.seed(1)
df2 <- tibble(
  log_init_true = rnorm(n, mean = 0, sd = sqrt(v_pop)),
  log_change_true = rnorm(n, mean = 0, sd = sqrt(v_ind)),        # Independent 0 change plus individual response variation
  # Later: Check if FINAL can be simulated to then derive change, and if this is equivalent
  log_final_true = log_init_true + log_change_true,           # Final = initial + change
  e_init = rnorm(n, mean = 0, sd = sqrt(v_err)),          # Random measurement error for initial values
  e_final = rnorm(n, mean = 0, sd = sqrt(v_err)),         # Random measurement error for final values
  log_init_obs = log_init_true + e_init,          # Add random measurement error to initial values
  log_final_obs = log_final_true + e_final,       # Add random measurement error to final values
  log_change_obs = log_final_obs - log_init_obs   # Observed change
)  

# Apply Kelly Price correction to true values and observed values
df2 <- df2 %>% mutate(
  log_change_true_adj = rttm.adj(log_init_true, log_final_true)[, 3],
  log_change_obs_adj = rttm.adj(log_init_obs, log_final_obs)[, 3]
)
  
# Update list of plots with scenario 2 data
plots2 <- map(plots1, ~ .x %+% df2)

# Create multipanel plot
plots2panel <- ggpubr::ggarrange(plotlist = plots2, nrow = 2, ncol = 4, labels = "AUTO")

# Display plots
plots2panel
```

* Confirms when variance in observed change is mostly due to measurement error (v_err >>> v_ind), Kelly Price adjustment recovers the correct zero relationship between observed change and initial (see **F**)

## 1.3: Variance in changes mostly due to individual differences 
### (v_pop == v_ind >>> v_err)

```{r s1.3, fig.width = 10, fig.height = 5}
# Set variances
v_pop <- 1
v_ind <- 1
v_err <- 0.001

# Simulate data
set.seed(1)
df3 <- tibble(
  log_init_true = rnorm(n, mean = 0, sd = sqrt(v_pop)),
  log_change_true = rnorm(n, mean = 0, sd = sqrt(v_ind)),        # Independent 0 change plus individual response variation
  # Later: Check if FINAL can be simulated to then derive change, and if this is equivalent
  log_final_true = log_init_true + log_change_true,           # Final = initial + change
  e_init = rnorm(n, mean = 0, sd = sqrt(v_err)),          # Random measurement error for initial values
  e_final = rnorm(n, mean = 0, sd = sqrt(v_err)),         # Random measurement error for final values
  log_init_obs = log_init_true + e_init,          # Add random measurement error to initial values
  log_final_obs = log_final_true + e_final,       # Add random measurement error to final values
  log_change_obs = log_final_obs - log_init_obs   # Observed change
)  

# Apply Kelly Price correction to true values and observed values
df3 <- df3 %>% mutate(
  log_change_true_adj = rttm.adj(log_init_true, log_final_true)[, 3],
  log_change_obs_adj = rttm.adj(log_init_obs, log_final_obs)[, 3]
)
  
# Update list of plots with scenario 3 data
plots3 <- map(plots1, ~ .x %+% df3)

# Create multipanel plot
plots3panel <- ggpubr::ggarrange(plotlist = plots3, nrow = 2, ncol = 4, labels = "AUTO")

# Display plots
plots3panel
```

* Confirms that when variance in observed change is mostly due to true variation in individual responses (v_ind >>> v_err), Kelly Price adjustment yields a strong spurious positive correlation between observed change and initial (see **F**)

# Scenario 2: True negative relationship between change and initial 

## 2.1: Equal variances 
### v_pop == v_ind == v_err

```{r s2.1, fig.width = 10, fig.height = 5}
# Set variances
v_pop <- 1
v_ind <- 1
v_err <- 1

# Set strength of true relationship
b <- -0.2

# Simulate data
set.seed(1)
df4 <- tibble(
  log_init_true = rnorm(n, mean = 0, sd = sqrt(v_pop)),
  log_change_true = b * scale(log_init_true)[,1] + rnorm(n, mean = 0, sd = sqrt(v_ind)),        # Independent 0 change plus individual response variation
  # Later: Check if FINAL can be simulated to then derive change, and if this is equivalent
  log_final_true = log_init_true + log_change_true,           # Final = initial + change
  e_init = rnorm(n, mean = 0, sd = sqrt(v_err)),          # Random measurement error for initial values
  e_final = rnorm(n, mean = 0, sd = sqrt(v_err)),         # Random measurement error for final values
  log_init_obs = log_init_true + e_init,          # Add random measurement error to initial values
  log_final_obs = log_final_true + e_final,       # Add random measurement error to final values
  log_change_obs = log_final_obs - log_init_obs   # Observed change
)  

# Apply Kelly Price correction to true values and observed values
df4 <- df4 %>% mutate(
  log_change_true_adj = rttm.adj(log_init_true, log_final_true)[, 3],
  log_change_obs_adj = rttm.adj(log_init_obs, log_final_obs)[, 3]
)
  
# Update list of plots with scenario 4 data
plots4 <- map(plots1, ~ .x %+% df4)

# Create multipanel plot
plots4panel <- ggpubr::ggarrange(plotlist = plots4, nrow = 2, ncol = 4, labels = "AUTO")

# Display plots
plots4panel
```

* Confirms that when there is a true negative relationship between initial and change, Kelly Price adjustment yields a false positive correlation between initial and change (see **C**), and a false zero correlation between observed initial and change (see **F**).

## 2.2: Variance in changes mostly due to measurement error 
### v_pop == v_err >>> v_ind

```{r s2.2, fig.width = 10, fig.height = 5}
# Set variances
v_pop <- 1
v_ind <- 0.001
v_err <- 1

# Set strength of true relationship
b <- -0.2

# Simulate data
set.seed(1)
df5 <- tibble(
  log_init_true = rnorm(n, mean = 0, sd = sqrt(v_pop)),
  log_change_true = b * scale(log_init_true)[,1] + rnorm(n, mean = 0, sd = sqrt(v_ind)),        # Independent 0 change plus individual response variation
  # Later: Check if FINAL can be simulated to then derive change, and if this is equivalent
  log_final_true = log_init_true + log_change_true,           # Final = initial + change
  e_init = rnorm(n, mean = 0, sd = sqrt(v_err)),          # Random measurement error for initial values
  e_final = rnorm(n, mean = 0, sd = sqrt(v_err)),         # Random measurement error for final values
  log_init_obs = log_init_true + e_init,          # Add random measurement error to initial values
  log_final_obs = log_final_true + e_final,       # Add random measurement error to final values
  log_change_obs = log_final_obs - log_init_obs   # Observed change
)  

# Apply Kelly Price correction to true values and observed values
df5 <- df5 %>% mutate(
  log_change_true_adj = rttm.adj(log_init_true, log_final_true)[, 3],
  log_change_obs_adj = rttm.adj(log_init_obs, log_final_obs)[, 3]
)
  
# Update list of plots with scenario 5 data
plots5 <- map(plots1, ~ .x %+% df5)

# Create multipanel plot
plots5panel <- ggpubr::ggarrange(plotlist = plots5, nrow = 2, ncol = 4, labels = "AUTO")

# Display plots
plots5panel
```

* Confirms when variance in observed change is mostly due to measurement error, Kelly Price adjustment yields true negative relationship between true change and initial (see **C**), but a false zero relationship between observed change and initial (see **F**).

## 2.3: Variance in changes mostly due to individual differences 
### v_pop == v_ind >>> v_err

```{r s2.3, fig.width = 10, fig.height = 5}
# Set variances
v_pop <- 1        # 1
v_ind <- 1        # 1
v_err <- 0.001    # 0.001

# Set strength of true relationship
b <- -0.2

# Simulate data
set.seed(1)
df6 <- tibble(
  log_init_true = rnorm(n, mean = 0, sd = sqrt(v_pop)),
  log_change_true = b * scale(log_init_true)[,1] + rnorm(n, mean = 0, sd = sqrt(v_ind)),        # Independent 0 change plus individual response variation
  # Later: Check if FINAL can be simulated to then derive change, and if this is equivalent
  log_final_true = log_init_true + log_change_true,           # Final = initial + change
  e_init = rnorm(n, mean = 0, sd = sqrt(v_err)),          # Random measurement error for initial values
  e_final = rnorm(n, mean = 0, sd = sqrt(v_err)),         # Random measurement error for final values
  log_init_obs = log_init_true + e_init,          # Add random measurement error to initial values
  log_final_obs = log_final_true + e_final,       # Add random measurement error to final values
  log_change_obs = log_final_obs - log_init_obs   # Observed change
)  

# Apply Kelly Price correction to true values and observed values
df6 <- df6 %>% mutate(
  log_change_true_adj = rttm.adj(log_init_true, log_final_true)[, 3],
  log_change_obs_adj = rttm.adj(log_init_obs, log_final_obs)[, 3]
)
  
# Update list of plots with scenario 6 data
plots6 <- map(plots1, ~ .x %+% df6)

# Create multipanel plot
plots6panel <- ggpubr::ggarrange(plotlist = plots6, nrow = 2, ncol = 4, labels = "AUTO")

# Display plots
plots6panel
```

+ Confirms that when variance in observed change is mostly due to true variation in individual responses, Kelly Price adjustment yields a false positive correlation between true change and initial (see **C**), and a false positive correlation between observed change and initial (see **F**).

# Simulation study of observed and adjusted correlation reliability

### Vary the true relationship *b* and variances *v_ind* and *v_err* to identify when the observed or Kelly Price adjusted relationship recovers the true relationship *b*

```{r parsim, fig.height = 5, fig.width = 10}
pars <- expand_grid(
  v_pop = 1,
  v_ind = seq(0, 3, 0.25),
  v_err = seq(0, 0.5, 0.05),
  b = c(-1, -0.5, -0.2, -0.1, 0, 0.1, 0.2, 0.5, 1)
)

test.cors <- function(v_pop, v_ind, v_err, b) {
  # Simulate data
  df <- tibble(
    log_init_true = rnorm(n, mean = 0, sd = sqrt(v_pop)),
    log_change_true = b * scale(log_init_true)[,1] + rnorm(n, mean = 0, sd = sqrt(v_ind)),   
    log_final_true = log_init_true + log_change_true,           # Final = initial + change
    e_init = rnorm(n, mean = 0, sd = sqrt(v_err)),          # Random measurement error for initial values
    e_final = rnorm(n, mean = 0, sd = sqrt(v_err)),         # Random measurement error for final values
    log_init_obs = log_init_true + e_init,          # Add random measurement error to initial values
    log_final_obs = log_final_true + e_final,       # Add random measurement error to final values
    log_change_obs = log_final_obs - log_init_obs   # Observed change
  )
  
  # Apply Kelly Price correction to observed values
  df <- df %>% mutate(
    log_change_obs_adj = rttm.adj(log_init_obs, log_final_obs)[, 3]
  )
  
  # Get true, observed, and adjusted correlations between change and initial
  bind_rows(.id = "cor.type",
        obs.b = tidy(lm(log_change_obs ~ scale(log_init_obs), data = df))[2,],
        adj.b = tidy(lm(log_change_obs_adj ~ scale(log_init_obs), data = df))[2,],
      obs.cor = tidy(cor.test(df$log_change_obs, df$log_init_obs)),
      adj.cor = tidy(cor.test(df$log_change_obs_adj, df$log_init_obs))
  ) %>% bind_cols(
    v_pop = v_pop,
    v_ind = v_ind,
    v_err = v_err,
    b = b
  )
}

# tidy(lm(log_change_obs ~ scale(log_init_obs), data = df))[2,]
# tidy(cor.test(df$log_change_obs, df$log_init_obs))
# plot(df$log_init_obs, df$log_change_obs)



# Run the function on all parameter combinations and bind results
results <- pmap_dfr(pars, test.cors) %>%
  select(v_pop, v_ind, v_err, b, cor.type, estimate, p.value) %>%
  mutate(outcome = case_when(
    p.value > 0.01 & b == 0 ~ "correct",
    p.value > 0.01 & b != 0 ~ "type2 - misses effect that does exist",
    p.value < 0.01 & sign(estimate) != sign(b) ~ "type1 - detects effect that does not exist",
    p.value < 0.01 & sign(estimate) == sign(b) ~ "correct"
  )) %>%
  mutate(dir = case_when(
    p.value > 0.01 ~ "none",
    p.value < 0.01 & sign(estimate) < 0 ~ "neg",
    p.value < 0.01 & sign(estimate) > 0 ~ "pos"
  ))


results %>%
  filter(cor.type %in% c("obs.cor", "adj.cor")) %>%
  ggplot(aes(y = v_err, x = v_ind)) +
    geom_point(aes(shape = dir, color = outcome, fill = outcome), size = 2) +
    facet_grid(cor.type ~ b, labeller = labeller(b = function(x) paste0("b = ", x))) +
    scale_shape_manual(values = c(25, 22, 24), labels = c("Negative", "Zero", "Positive")) +
    scale_color_manual(values = c("#0072B2", "#E69F00", "#999999")) +
    scale_fill_manual(values = c("#0072B2", "#E69F00", "#999999")) +
    scale_x_continuous(breaks = seq(0, 4, 0.5)) +
    scale_y_continuous(breaks = seq(0, 0.5, 0.1)) +
    guides(shape = guide_legend(title = "Effect sign"),
           color = guide_legend(title = "Outcome")) +
  theme_bw() +
  theme(legend.position = "bottom",
        axis.text.x = element_text(angle = 90, vjust = 0.5))




```

* **When true relationship is zero:**     
  + Observed correlation is nearly always *spuriously negative* due to regression to the mean/mathematical coupling unless measurement error is very low (≤0.1ish)
  + Adjusted relationship is *spuriously positive* when v_ind > v_err     
  + Adjusted relationship is *correct* when v_err > v_ind     
* **When true relationship is negative:**     
  + Observed relationship is always negative (*correct*), but *spuriously overestimated*     
  + Adjusted relationship is *spuriously positive* for v_ind > v_err     
  + Adjusted relationship is *spuriously zero* when v_err > v_ind (and most always as b decreases)     
* **When true relationship is positive:**      
  + Observed relationship is *spuriously negative* when v_err is > 0.1-0.5 (increasing as b increases), *spuriously zero* at intermediate v_err values, but *correct* when v_err is very low  
  + Adjusted relationship is positive in most cases (*correct*), but probably *spuriously overestimated*


# Dataset 1: *P. damicornis* symbiont changes

from Cunning and Baker 2013

### Import and tidy data
```{r pdam_import_tidy}
# Import and tidy Pdam data
pdam0 <- read_csv("PdamRbleaching.csv")

pdam <- pdam0 %>%
  mutate(log_init_raw = log(juntotal),
         log_final_raw = log(augtotal),
         log_change = log_final_raw - log_init_raw)


# Remove symbiont effect

## Calculate grand mean
grand_mean_init <- mean(pdam$log_init_raw)
grand_mean_final <- mean(pdam$log_final_raw)

## Add sym-group residuals to grand mean
pdam <- pdam %>%
  group_by(sym) %>%
  mutate(group_resid_init = log_init_raw - mean(log_init_raw),
         group_resid_final = log_final_raw - mean(log_final_raw)) %>%
  ungroup() %>%
  mutate(log_init_obs = grand_mean_init + group_resid_init,
         log_final_obs = grand_mean_final + group_resid_final,
         log_change_obs = log_final_obs - log_init_obs) %>%
  select(colony, log_init_obs, log_final_obs, log_change_obs)

# Kelly Price adjustment
pdam <- pdam %>% mutate(
  log_change_obs_adj = rttm.adj(log_init_obs, log_final_obs)[, 3]
)

save(pdam, file = "pdam.RData")
```

### Observed and adjusted relationships in *P. damicornis* study
```{r pdam_plots, fig.width = 10, fig.height = 5}
# ## Compute new plot limit values
pdam_initfinal <- pdam %>%
  select(log_init_obs, log_final_obs) %>%
  summarise(
    min = min(c_across(everything()), na.rm = TRUE),
    max = max(c_across(everything()), na.rm = TRUE)
  )
pdam_initchange <- pdam %>%
  select(log_change_obs) %>%
  summarise(
    min = min(c_across(everything()), na.rm = TRUE),
    max = max(c_across(everything()), na.rm = TRUE)
  )

pdam_plots <- list(
  chiolero_plot(pdam, log_init_obs, log_final_obs),
  p6 %+% pdam + coord_fixed(xlim = c(pdam_initfinal$min, pdam_initfinal$max),
                            ylim = c(pdam_initfinal$min, pdam_initfinal$max)),
  p7 %+% pdam + coord_fixed(xlim = c(pdam_initfinal$min, pdam_initfinal$max),
                            ylim = c(pdam_initchange$min, pdam_initchange$max)),
  p8 %+% pdam + coord_fixed(xlim = c(pdam_initfinal$min, pdam_initfinal$max),
                            ylim = c(pdam_initchange$min, pdam_initchange$max))
)

ggpubr::ggarrange(plotlist = pdam_plots, nrow = 1, ncol = 4, labels = "AUTO")
```

* Observed relationship is negative (interpreted as density-dependent bleaching in Cunning and Baker 2013)
* Kelly and Price adjusted relationship is zero, suggesting it could entirely reflect regression to the mean
* Which is it? Look at which simulated scenarios reproduce this observed relationship...

### Which simulations produce the observed relationship from *P. damicornis* study?
```{r pdam_scenarios}
# Which combinations of b, v_ind, and v_err could plausibly produce both:
# the observed slope of change vs. initial
# the observed variance in change?
# You do this by simulating datasets across a grid, then keeping only those combinations where:
# 
# The observed slope in the simulated dataset matches the observed slope in your real data
# The predicted variance in change (b^2 + v_ind + 2*v_err) matches the variance in your real data
# This filtering step is what makes your inference about b_true and v_ind data-driven, rather than just based on slope matching.
# Then we look at the TRUE underlying relationship b in the set of plausible simulations.

# Slope and variance estimation from Pdam dataset 
pdam_mod <- lm(log_change_obs ~ scale(log_init_obs), data = pdam)  # scaled to unit variance to match simulations
b_obs <- coef(pdam_mod)[[2]]      # -0.37 (observed slope)
b_obs_ci <- confint(pdam_mod)[2,]   # 95% confidence interval for observed slope
# Variance in initial observed trait values and observed individual changes (on absolute/trait scale)
var_x_abs <- var(pdam$log_init_obs, na.rm = TRUE)
var_y_abs <- var(pdam$log_change_obs, na.rm = TRUE)

# Measurement error variance (from qPCR technical replicates; see pdam_variance.R)
v_err_abs <- 0.0231325     # on trait scale (logSH)
v_pop_abs <- var_x_abs - v_err_abs       # Initial true trait value variance (subtract measurement error var)
v_err_rel <- v_err_abs / v_pop_abs       # Measurement error variance on relative scale
                                         # i.e., relative to v_pop since v_pop standardized to 1 in simulations

# Estimate v_ind from data based on known v_err and observed variance in change.
# b_obs is from standardized log_init_obs, so we multiply by var_x_abs to put it back on the unscaled (absolute) scale
v_ind_est_abs <- var_y_abs - (b_obs^2 * var_x_abs) - 2 * v_err_abs
v_ind_est_rel <- v_ind_est_abs / v_pop_abs
# 1.57 rel to v_pop. let's search 0 to 4 in simulations just to include very broad search.

# Observed variance in change, rescaled to simulation scale (v_pop = 1)
var_y_rel <- var_y_abs / v_pop_abs
# Bootstrap confidence interval for variance in observed change over time
library(boot)
boot_fun <- function(data, indices) {
  d <- data[indices, ]
  var(d$log_change_obs, na.rm = TRUE)
}
set.seed(123)
boot_out <- boot(pdam, statistic = boot_fun, R = 1000)
boot_ci <- boot.ci(boot_out, type = "perc")

# Convert to relative scale
var_y_abs_low  <- boot_ci$percent[4]
var_y_abs_high <- boot_ci$percent[5]

var_y_rel_low  <- var_y_abs_low  / v_pop_abs
var_y_rel_high <- var_y_abs_high / v_pop_abs









# Run new simulations with known Pdam v_err to identify all possible true relationships given observed slope
pdam_pars <- expand_grid(
  v_pop = 1,                            # Set trait variance to 1 (relative)
  v_ind = seq(0, 4, 0.1),               # Test broad v_ind (relative to v_pop = 1; Pdam estimate = 1.57)
  v_err = seq(0.06, 0.27, 0.01),        # Meas. err. variance on rel. scale (95% CI of v_err_abs / v_pop_abs)
  b = seq(-1, 1, 0.01)                  # Test all unknown true relationships
)



# Run the function on all parameter combinations and bind results
library(future)
library(furrr)
plan(multisession)
pdam_sim_all <- future_pmap_dfr(
  pdam_pars,
  test.cors,
  .options = furrr_options(seed = TRUE, packages = "broom")  # ensures broom is loaded on each worker
) %>%
  select(v_pop, v_ind, v_err, b, cor.type, estimate, p.value)

# Get all possible true b simulations that could produce pdam observed b [±95CI]
#tolerance <- var_y_rel * 0.1
pdam_sim_possible <- pdam_sim_all %>%
  pivot_wider(names_from = cor.type, values_from = c(estimate, p.value)) %>%
  mutate(
    # Keep only simulations where:
    # 1. Simulated observed slope (estimate_obs.b) lies within the 95% CI from real data
    slope_match = estimate_obs.b > b_obs_ci[1] & 
                  estimate_obs.b < b_obs_ci[2],
    # 2. Simulated total variance in change (b^2 + v_ind + 2 * v_err) lies within the bootstrapped 95% CI of var_y_rel
    var_match = (v_ind + b^2 + 2 * v_err >= var_y_rel_low) & 
                (v_ind + b^2 + 2 * v_err <= var_y_rel_high)
  ) %>%
  filter(slope_match, var_match)



# Plot true and KP-adjusted slopes for possible scenarios consistent with Pdam study
pdam_sim_possible %>%
  select(v_ind, b, estimate_obs.b, estimate_adj.b) %>%
  pivot_longer(cols = c(b, estimate_obs.b, estimate_adj.b)) %>%
  ggplot(aes(x = value)) +
  geom_histogram(aes(fill = name), position = "identity", 
                 binwidth = 0.01, alpha = 0.5)


mean(pdam_sim_possible$b)     # likely true slope from simulations given observed
quantile(pdam_sim_possible$b, c(0.025, 0.975))
b_obs    # observed slope is more negative than likely true slope
b_obs_ci

# # # Heatmap of most likely parameter combos
# ggplot(pdam_sim_possible, aes(x = b, y = v_ind)) +
#   geom_bin2d(binwidth = c(0.02, 0.1)) +
#   scale_fill_viridis_c(option = "C") +
#   labs(
#     title = "Parameter combinations consistent with observed slope and variance",
#     x = "True slope (b)", y = "True individual variance (v_ind)"
#   )
```




# Dataset 2: *A. cervicornis* growth

from Million et al.

### Import and tidy data
```{r mill_import_tidy}
# Import data
mill <- read_csv("subMil1.csv")       # Generated in Carly's script

# Tidy and filter data
mill <- mill %>%
  filter(size > 0) %>%  # there are four rows where size = 0, filter out because will generate Inf
  filter(InitialSize > 2) %>%    # cutoff of 1 gives adjusted marginal sig, cutoff 2 nonsig
  mutate(log_init_obs = log(InitialSize),
         log_final_obs = log(size),
         log_change_obs = log_final_obs - log_init_obs)

# Kelly Price adjustment
mill <- mill %>% mutate(
  log_change_obs_adj = rttm.adj(log_init_obs, log_final_obs)[, 3]
)
```

### Observed and adjusted relationships in *A. cervicornis* study
```{r mill_plots, fig.width = 10, fig.height = 5}
# ## Compute new plot limit values
mill_initfinal <- mill %>%
  select(log_init_obs, log_final_obs) %>%
  summarise(
    min = min(c_across(everything()), na.rm = TRUE),
    max = max(c_across(everything()), na.rm = TRUE)
  )
mill_initchange <- mill %>%
  select(log_change_obs) %>%
  summarise(
    min = min(c_across(everything()), na.rm = TRUE),
    max = max(c_across(everything()), na.rm = TRUE)
  )

### Observed and adjusted relationships in *A. cervicornis* study
mill_plots <- list(
  chiolero_plot(mill, log_init_obs, log_final_obs),
  p6 %+% mill + coord_fixed(xlim = c(mill_initfinal$min, mill_initfinal$max),
                            ylim = c(mill_initfinal$min, mill_initfinal$max)),
  p7 %+% mill + coord_fixed(xlim = c(mill_initfinal$min, mill_initfinal$max),
                            ylim = c(mill_initchange$min, mill_initchange$max)),
  p8 %+% mill + coord_fixed(xlim = c(mill_initfinal$min, mill_initfinal$max),
                            ylim = c(mill_initchange$min, mill_initchange$max))
)
  
ggpubr::ggarrange(plotlist = mill_plots, nrow = 1, ncol = 4, labels = "AUTO")
```


### Which simulations produce the observed relationship from *P. damicornis* study?
```{r mill_scenarios}
# Which combinations of b, v_ind, and v_err could plausibly produce both:
# the observed slope of change vs. initial
# the observed variance in change?
# You do this by simulating datasets across a grid, then keeping only those combinations where:
# 
# The observed slope in the simulated dataset matches the observed slope in your real data
# The predicted variance in change (b^2 + v_ind + 2*v_err) matches the variance in your real data
# This filtering step is what makes your inference about b_true and v_ind data-driven, rather than just based on slope matching.
# Then we look at the TRUE underlying relationship b in the set of plausible simulations.

# Slope and variance estimation from mill dataset 
mill_mod <- lm(log_change_obs ~ scale(log_init_obs), data = mill)  # scaled to unit variance to match simulations
b_obs <- coef(mill_mod)[[2]]      # -0.37 (observed slope)
b_obs_ci <- confint(mill_mod)[2,]   # 95% confidence interval for observed slope
# Variance in initial observed trait values and observed individual changes (on absolute/trait scale)
var_x_abs <- var(mill$log_init_obs, na.rm = TRUE)
var_y_abs <- var(mill$log_change_obs, na.rm = TRUE)

# Measurement error variance 
v_err_abs <- 0.01301945     # on trait scale (logTLE)
v_pop_abs <- var_x_abs - v_err_abs       # Initial true trait value variance (subtract measurement error var)
v_err_rel <- v_err_abs / v_pop_abs       # Measurement error variance on relative scale
                                         # i.e., relative to v_pop since v_pop standardized to 1 in simulations

# Estimate v_ind from data based on known v_err and observed variance in change.
# b_obs is from standardized log_init_obs, so we multiply by var_x_abs to put it back on the unscaled (absolute) scale
v_ind_est_abs <- var_y_abs - (b_obs^2 * var_x_abs) - 2 * v_err_abs
v_ind_est_rel <- v_ind_est_abs / v_pop_abs
# 0.23 rel to v_pop. let's search 0 to 1 in simulations just to include very broad search.

# Observed variance in change, rescaled to simulation scale (v_pop = 1)
var_y_rel <- var_y_abs / v_pop_abs
# Bootstrap confidence interval for variance in observed change over time
library(boot)
boot_fun <- function(data, indices) {
  d <- data[indices, ]
  var(d$log_change_obs, na.rm = TRUE)
}
set.seed(123)
boot_out <- boot(mill, statistic = boot_fun, R = 1000)
boot_ci <- boot.ci(boot_out, type = "perc")

# Convert to relative scale
var_y_abs_low  <- boot_ci$percent[4]
var_y_abs_high <- boot_ci$percent[5]

var_y_rel_low  <- var_y_abs_low  / v_pop_abs
var_y_rel_high <- var_y_abs_high / v_pop_abs









# Run new simulations with known mill v_err to identify all possible true relationships given observed slope
mill_pars <- expand_grid(
  v_pop = 1,                            # Set trait variance to 1 (relative)
  v_ind = seq(0, 1, 0.025),               # Test broad v_ind (relative to v_pop = 1; mill estimate = 1.57)
  v_err = seq(0, 0.05, 0.005),        # Meas. err. variance on rel. scale (95% CI of v_err_abs / v_pop_abs)
  b = seq(-1, 1, 0.01)                  # Test all unknown true relationships
)



# Run the function on all parameter combinations and bind results
library(future)
library(furrr)
plan(multisession)
mill_sim_all <- future_pmap_dfr(
  mill_pars,
  test.cors,
  .options = furrr_options(seed = TRUE, packages = "broom")  # ensures broom is loaded on each worker
) %>%
  select(v_pop, v_ind, v_err, b, cor.type, estimate, p.value)

# Get all possible true b simulations that could produce mill observed b [±95CI]
mill_sim_possible <- mill_sim_all %>%
  pivot_wider(names_from = cor.type, values_from = c(estimate, p.value)) %>%
  mutate(
    # Keep only simulations where:
    # 1. Simulated observed slope (estimate_obs.b) lies within the 95% CI from real data
    slope_match = estimate_obs.b > b_obs_ci[1] & 
                  estimate_obs.b < b_obs_ci[2],
    # 2. Simulated total variance in change (b^2 + v_ind + 2 * v_err) lies within the bootstrapped 95% CI of var_y_rel
    var_match = (v_ind + b^2 + 2 * v_err >= var_y_rel_low) & 
                (v_ind + b^2 + 2 * v_err <= var_y_rel_high)
  ) %>%
  filter(slope_match, var_match)



# Plot true and KP-adjusted slopes for possible scenarios consistent with mill study
mill_sim_possible %>%
  select(v_ind, b, estimate_obs.b, estimate_adj.b) %>%
  pivot_longer(cols = c(b, estimate_obs.b, estimate_adj.b)) %>%
  ggplot(aes(x = value)) +
  geom_histogram(aes(fill = name), position = "identity", 
                 binwidth = 0.01, alpha = 0.5)


mean(mill_sim_possible$b)     # likely true slope from simulations given observed
quantile(mill_sim_possible$b, c(0.025, 0.975))
b_obs    # observed slope is more negative than likely true slope
b_obs_ci

# # # Heatmap of most likely parameter combos
# ggplot(mill_sim_possible, aes(x = b, y = v_ind)) +
#   geom_bin2d(binwidth = c(0.01, 0.025)) +
#   scale_fill_viridis_c(option = "C") +
#   labs(
#     title = "Parameter combinations consistent with observed slope and variance",
#     x = "True slope (b)", y = "True individual variance (v_ind)"
#   )
```
