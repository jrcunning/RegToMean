---
title: "Regression to the mean and the detection of true relationships between change and initial values"
author: "Ross Cunning"
date: "2025-09-03"
output:
  html_document:
    toc: true
    toc_float: true
    toc_depth: 2
    theme: default  # or "lumen", "flatly", etc.
editor_options: 
  chunk_output_type: console
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, message = FALSE, warning = FALSE, cache = TRUE)
```

# Objectives

Use simulated datasets to test conditions under which mathematical coupling, regression to the mean, and statistical adjustments to control for these artifacts could:  

1. correctly recover the true relationship between change and initial values;
2. produce a spurious relationship when there is not one;  
3. fail to recover a true relationship when there is one.  

Simulations have either no relationship between change and initial values ([Scenario 1](#Scenario 1: No relationship between change and initial)), or a true relationship specified by parameter *b* ([Scenario 2](# Scenario 2: True negative relationship between change and initial)), and include three sources of variance:  

1. Variance among individuals in the true value (population variance; *v_pop*)  
2. Variance in within-individual changes over time (individual-level variance; *v_ind*)  
3. Variance due to noise or imprecision in measurement (error variance; *v_err*))       

Parameter space then is explored through a [simulation study](# Simulation study of observed and adjusted correlation reliability) to find which combinations of *b*, *v_ind*, and *v_err* give rise to true or spurious relationships based on observed values or those corrected with the Kelly Price adjustment for regression to the mean.

Following simulations, analyze two coral datasets describing [symbiont changes in *P. damicornis*](# Dataset 1: *P. damicornis* symbiont changes) and [growth in *A. cervicornis*](# Dataset 2: *A. cervicornis* growth) and assess the likelihood of the true relationship given observed values in the context of simulations.



# Setup
```{r libraries, echo = FALSE}
# Load libraries
library(tidyverse)
library(ggpubr)
library(ggpmisc)
library(broom)
library(patchwork)
library(grid)
library(boot)
library(future)
library(furrr)
```

```{r functions, echo = FALSE}
# Define helper functions

# Function to correct for spurious slopes, from Kelly and Price 2005
rttm.adj <- function(m1, m2){
  raw.growth<-m2-m1
  vart<-var.test(m1,m2,paired = T) ## variances equal? 
  vpv<-vart$p.value # var.test p value
  m1m2cor<-cor.test(m1, m2) # test correlation between m1 and m2 
  rho<-m1m2cor$estimate # correlation coefficient between m1 and m2 
  m1sd<-sd(m1) # m1 sd
  m2sd<-sd(m2) # m2 sd
  m1v<-var(m1) # m1 var
  m2v<-var(m2) # m2 var
  m1m<-mean(m1) # m1 mean
  m2m<-mean(m2) # m2 mean
  pm<-mean(raw.growth)
  rho2<-(2*rho*m1sd*m2sd)/(m1v+m2v) # adjusted correlation coefficient used if variances are equal
  rhof<-ifelse(vpv <= 0.05, rho, rho2) # which rho is used for dstar calculation is based on variance comparison
  dstar<-(rhof*(m1-m1m)-(m2-m2m))*-1 # adjustment values. Multiply by -1 to flip sign because Kelly and Price based on plasticity as m1-m2, not m2-m1 as in most thermal tolerance estimates
  adj.growth <- pm+dstar # corrected plasticity. 
  out<-as.data.frame(cbind(raw.growth, dstar, adj.growth)) 
  return(out)
}

# Function go get Blomqvist-adjusted slope
blomqvist_b <- function(b_obs, k) {
  (b_obs + k) / (1 - k)
}


# Function to adjust y values using Blomqvist-adjusted slope
blomqvist_pseudo_exact <- function(x, y, b_blom) {
  x <- as.numeric(x)
  y <- as.numeric(y)
  e <- resid(lm(y ~ x))                 # orthogonal to 1 and x
  a <- mean(y) - b_blom * mean(x)       # preserve mean
  a + b_blom * x + e
}



## GGplot var.test
stat_var_test <- function(mapping = NULL, data = NULL,
                          method = var.test,
                          xvar = NULL, yvar = NULL,
                          label.x = NULL, label.y = NULL,
                          position = "identity",
                          na.rm = FALSE, show.legend = NA,
                          inherit.aes = TRUE, ...) {

  layer(
    stat = StatVarTest,
    data = data,
    mapping = mapping,
    geom = "text",
    position = position,
    show.legend = show.legend,
    inherit.aes = inherit.aes,
    params = list(
      method = method,
      xvar = xvar,
      yvar = yvar,
      label.x = label.x,
      label.y = label.y,
      na.rm = na.rm,
      ...
    )
  )
}

StatVarTest <- ggproto("StatVarTest", Stat,
  required_aes = c("x", "y"),
  compute_group = function(data, scales, method, xvar, yvar, label.x, label.y) {
    #cat("Number of rows in compute_group:", nrow(data), "\n")
    # Perform the test
    vtest <- method(data$x, data$y)

    # Create label
    label <- paste0("v_i:v_f = ", signif(vtest$estimate, 3),
                    ",\np = ", signif(vtest$p.value, 3))

    # Determine label position
    data.frame(
      x = label.x %||% mean(range(data$x, na.rm = TRUE)),
      y = label.y %||% max(data$y, na.rm = TRUE),
      label = label
    )
  }
)



# Chiolero plot function
chiolero_plot <- function(df, init_var, final_var, label = NULL) {
  init_sym <- ensym(init_var)
  final_sym <- ensym(final_var)
  init_name <- as_label(init_sym)
  final_name <- as_label(final_sym)

  # Get min and max across both variables
  y_range <- range(c(pull(df, !!init_sym), pull(df, !!final_sym)), na.rm = TRUE)

  ggplot(df, aes(x = 1, xend = 2, y = !!init_sym, yend = !!final_sym)) +
    geom_segment(alpha = 0.1) +
    geom_point(aes(x = 1, y = !!init_sym), size = 1.5, alpha = 0.2) +
    geom_point(aes(x = 2, y = !!final_sym), size = 1.5, alpha = 0.2) +
    scale_y_continuous(limits = y_range) +
    scale_x_continuous(breaks = c(1, 2), labels = c(init_name, final_name),
                       expand = c(0.15, 0.15)) +
    labs(x = "timepoint", y = "value") +
    stat_var_test(
      mapping = aes(x = !!init_sym, y = !!final_sym, group = 1),
      label.x = -Inf,
      label.y = Inf,
      hjust = -0.1, vjust = 1
    )
}


# Run simulation scenario function
run_simulation_scenario <- function(n, b, v_ind, v_err, v_pop = 1, base_plots) {
  set.seed(1)
  
  # Simulate data
  df <- tibble(
    log_init_true = rnorm(n, mean = 0, sd = sqrt(v_pop)),
    log_change_true = b * log_init_true +
                      rnorm(n, mean = 0, sd = sqrt(v_ind)),
    log_final_true = log_init_true + log_change_true,
    e_init = rnorm(n, mean = 0, sd = sqrt(v_err)),
    e_final = rnorm(n, mean = 0, sd = sqrt(v_err)),
    log_init_obs = log_init_true + e_init,
    log_final_obs = log_final_true + e_final,
    log_change_obs = log_final_obs - log_init_obs
  )
  
  # Kelly-Price corrections
  df <- df %>%
    mutate(
      log_change_true_adj = rttm.adj(log_init_true, log_final_true)[, 3],
      log_change_obs_adj  = rttm.adj(log_init_obs, log_final_obs)[, 3]
    )
  
  # Update all plots using this new dataset
  updated_plots <- map(base_plots, ~ .x %+% df)
  
  # Return both the data and plots (if you want to access both)
  list(data = df, plots = updated_plots)
}


# Annotated scenario plot function
library(patchwork)
library(grid)

annotated_panel_plot <- function(plots) {
  if (length(plots) != 9) {
    stop("You must supply a list of exactly 9 plots.")
  }

  # Pull parameter values from global environment
  v_pop <- get("v_pop", envir = .GlobalEnv)
  v_ind <- get("v_ind", envir = .GlobalEnv)
  v_err <- get("v_err", envir = .GlobalEnv)
  b     <- get("b", envir = .GlobalEnv)

  # Build dynamic title text
  title_text <- paste0("b = ", signif(b, 3),
                       "; v_ind = ", signif(v_ind, 3), 
                       "; v_err = ", signif(v_err, 3))

  # Arrange plots in 2Ã—4 grid
  plot_grid <- cowplot::plot_grid(
    plots[[1]], plots[[2]], plots[[3]], plots[[4]], NULL,
    plots[[5]], plots[[6]], plots[[7]], plots[[8]], plots[[9]],
    ncol = 5,
    align = "hv"
  )

  # Add spacer row for annotations
  plot_with_spacer <- plot_spacer() / plot_grid +
    plot_layout(heights = c(0.05, 1))

  # Print the plot layout
  print(plot_with_spacer)

  # Top-left title
  grid.text(title_text, x = 0, y = 0.975, just = "left",
            gp = gpar(fontsize = 13, fontface = "bold"))

  # # Column headers
  # grid.text("Initial & Final Values", x = 0.2, y = 0.92, just = "left",
  #           gp = gpar(fontsize = 13, fontface = "bold"))
  # grid.text("Change vs. Initial", x = 0.57, y = 0.92, just = "left",
  #           gp = gpar(fontsize = 13, fontface = "bold"))
  # grid.text("K-P Adjusted", x = 0.83, y = 0.92, just = "left",
  #           gp = gpar(fontsize = 13, fontface = "bold"))
  # grid.text("Blomq Adjusted", x = 0.88, y = 0.92, just = "left",
  #           gp = gpar(fontsize = 13, fontface = "bold"))
  # 
  # # Row labels
  # grid.text("True values", x = 0.01, y = 0.675, rot = 90,
  #           gp = gpar(fontsize = 13, fontface = "bold"))
  # grid.text("Observed values", x = 0.01, y = 0.25, rot = 90,
  #           gp = gpar(fontsize = 13, fontface = "bold"))
}
```




# Scenario 1: No relationship between change and initial

## 1.1: Measurement error dominates variance in change

```{r s1.1, fig.width = 10, fig.height = 5.5}
# Set number of individuals for simulations
n <- 1000

# Set variances
v_pop <- 1         # true variance in initial trait values

# Total change variance is 1 (= v_ind + 2 * v_err); error contributes 99% of total
v_ind <- 0.01
v_err <- 0.495    # 0.99/2

# No true relationship to initial value
b <- 0

run_simulation_scenario <- function(
  n, b, v_ind, v_err, v_pop = 1, seed = 1
) {
  set.seed(seed)

  # --- 1) Simulate ---
  df1 <- tibble(
    log_init_true  = rnorm(n, 0, sqrt(v_pop)),
    log_change_true = b * log_init_true + rnorm(n, 0, sqrt(v_ind)),
    log_final_true = log_init_true + log_change_true,
    e_init  = rnorm(n, 0, sqrt(v_err)),
    e_final = rnorm(n, 0, sqrt(v_err)),
    log_init_obs  = log_init_true  + e_init,
    log_final_obs = log_final_true + e_final,
    log_change_obs = log_final_obs - log_init_obs
  )

  # --- 2) Observed slope + CI ---
  fit_obs <- lm(log_change_obs ~ log_init_obs, data = df1)
  b_obs   <- coef(fit_obs)[2]
  b_obs_confint <- confint(fit_obs)[2, ]

  # --- 3) Kellyâ€“Price (point + bootstrap CI) ---
  kp_full <- rttm.adj(df1$log_init_obs, df1$log_final_obs)
  b_kp_adj <- coef(lm(kp_full$adj.growth ~ df1$log_init_obs))[2]

  set.seed(seed + 100)
  kp_boot <- boot::boot(
    data = df1,
    statistic = function(d, i) {
      di <- d[i, , drop = FALSE]
      kp <- rttm.adj(di$log_init_obs, di$log_final_obs)
      coef(lm(kp$adj.growth ~ di$log_init_obs))[2]
    },
    R = 2000
  )
  b_kp_adj_confint_boot <- stats::quantile(kp_boot$t, c(0.025, 0.975), na.rm = TRUE)

  # --- 4) Blomqvist (point + bootstrap CI) ---
  k <- v_err / (v_pop + v_err)
  b_blomq_adj <- blomqvist_b(b_obs, k)

  # pseudo-exact yâ€™ (used only for plotting panel, not for CI)
  df1 <- df1 %>%
    dplyr::mutate(
      log_change_kp_adj    = kp_full$adj.growth,
      log_change_blomq_adj = blomqvist_pseudo_exact(log_init_obs, log_final_obs, b_blomq_adj)
    )

  set.seed(seed + 200)
  blomqvist_boot <- boot::boot(
    data = df1,
    statistic = function(d, i) {
      di <- d[i, , drop = FALSE]
      b_i <- coef(lm(log_change_obs ~ log_init_obs, data = di))[2]
      blomqvist_b(b_i, k)
    },
    R = 2000
  )
  b_blomq_adj_confint_boot <- stats::quantile(blomqvist_boot$t, c(0.025, 0.975), na.rm = TRUE)

  # --- 5) Slopes tibble ---
  slopes_tbl <- tibble::tibble(
    method   = factor(c("Obs", "K-P", "Blomq"), levels = c("Obs", "K-P", "Blomq")),
    estimate = c(b_obs, b_kp_adj, b_blomq_adj),
    ci_lower = c(b_obs_confint[1], b_kp_adj_confint_boot[1], b_blomq_adj_confint_boot[1]),
    ci_upper = c(b_obs_confint[2], b_kp_adj_confint_boot[2], b_blomq_adj_confint_boot[2])
  )

  # --- 6) Axis ranges for plots ---
  max_abs_initfinal <- df1 %>%
    dplyr::select(log_init_true, log_final_true, log_init_obs, log_final_obs) %>%
    dplyr::summarise(max_abs = max(abs(dplyr::c_across(everything())), na.rm = TRUE)) %>%
    dplyr::pull(max_abs)

  max_abs_initchange <- df1 %>%
    dplyr::select(log_init_true, log_change_true, log_init_obs, log_change_obs) %>%
    dplyr::summarise(max_abs = max(abs(dplyr::c_across(everything())), na.rm = TRUE)) %>%
    dplyr::pull(max_abs)

  # --- 7) Plots (uses your existing helpers: chiolero_plot, annotated_panel_plot) ---
  p1 <- chiolero_plot(df1, log_init_true,  log_final_true)  + ggtitle("True")
  p2 <- chiolero_plot(df1, log_init_obs,   log_final_obs)   + ggtitle("Observed")

  p3 <- ggplot2::ggplot(df1, ggplot2::aes(x = log_init_true, y = log_final_true)) +
    ggplot2::geom_point(alpha = 0.2) +
    ggplot2::geom_abline(slope = 1, intercept = 0, linetype = "dashed") +
    ggpubr::stat_cor(label.x = -Inf, label.y = Inf, hjust = -0.1, vjust = 1.5) +
    ggplot2::geom_smooth(method = "lm", se = FALSE) +
    ggplot2::coord_fixed(xlim = c(-max_abs_initfinal, max_abs_initfinal),
                         ylim = c(-max_abs_initfinal, max_abs_initfinal)) +
    ggtitle("True")

  p4 <- p3 + ggplot2::aes(x = log_init_obs, y = log_final_obs) + ggtitle("Observed")

  p5 <- ggplot2::ggplot(df1, ggplot2::aes(x = log_init_true, y = log_change_true)) +
    ggplot2::geom_point(alpha = 0.2) +
    ggplot2::geom_abline(slope = 0, intercept = 0, linetype = "dashed") +
    ggpubr::stat_cor(label.x = -Inf, label.y = Inf, hjust = -0.1, vjust = 1.5) +
    ggplot2::geom_smooth(method = "lm", se = FALSE) +
    ggplot2::coord_fixed(xlim = c(-max_abs_initchange, max_abs_initchange),
                         ylim = c(-max_abs_initchange, max_abs_initchange)) +
    ggtitle("True")

  p6 <- p5 + ggplot2::aes(x = log_init_obs, y = log_change_obs) + ggtitle("Observed")
  p7 <- p5 + ggplot2::aes(x = log_init_obs, y = log_change_kp_adj) + ggtitle("Kelly-Price")
  p8 <- p5 + ggplot2::aes(x = log_init_obs, y = log_change_blomq_adj) + ggtitle("Blomqvist")

  p9 <- ggplot2::ggplot(slopes_tbl, ggplot2::aes(x = method, y = estimate)) +
    ggplot2::geom_point() +
    ggplot2::geom_errorbar(ggplot2::aes(ymin = ci_lower, ymax = ci_upper), width = 0.05) +
    ggplot2::geom_hline(yintercept = b, linetype = 2) +
    ggplot2::labs(x = NULL, y = "Slope")

  plots1 <- list(p1, p2, p3, p4, p5, p6, p7, p8, p9)

  # Optional: assemble your panel externally with annotated_panel_plot(plots1)
  list(
    data = df1,
    slopes = slopes_tbl,
    plots = plots1,
    boot = list(kelly_price = kp_boot, blomqvist = blomqvist_boot)
  )
}

s1.1_results <- run_simulation_scenario(
  n = n, b = b, v_ind = v_ind, v_err = v_err, v_pop = v_pop)

annotated_panel_plot(s1.1_results$plots)
```

* **G** confirms spurious negative relationship between observed change and initial is due to measurement error (compare to **C**, true values)
* **H** confirms that Kelly and Price correction yields the true zero relationship


## 1.2: Individual variance dominates
```{r s1.2_new, fig.width = 10, fig.height = 5.5}
# Set variances
v_pop <- 1
# Total change variance is 1 (= v_ind + 2 * v_err); v_ind contributes 99% of total
v_ind <- 0.99
v_err <- 0.005    # 0.01/2
# No true relationship to initial value
b <- 0

s1.2_results <- run_simulation_scenario(
  n = n, b = b, v_ind = v_ind, v_err = v_err, v_pop = v_pop)

# Plot it
annotated_panel_plot(s1.2_results$plots)
```

* **G** confirms that no spurious relationship arises when changes are driven by individual variance instead of measurement error (there is no regression to the mean).
* **D and H** confirm that Kelly-Price adjustment interprets the individual variance as measurement error and attempts to correct for regression to the mean, yielding a spurious positive relationship between change and initial values.

## 1.3: Equal individual variance and measurement error

```{r s1.3, fig.width = 10, fig.height = 5.5}
# Set variances
v_pop <- 1

# Total change variance is 1 (= v_ind + 2 * v_err); v_ind and 2 * v_err each contribute 50%
v_ind <- 0.5
v_err <- 0.25    # 0.5/2
# No true relationship to initial value
b <- 0

s1.3_results <- run_simulation_scenario(
  n = n, b = b, v_ind = v_ind, v_err = v_err, v_pop = v_pop)

# Plot it
annotated_panel_plot(s1.3_results$plots)
```

* **G** confirms again that regression to the mean due to measurement error produces spurious negative relationship between change and initial values.
* But, **D and H** show that Kelly and Price over-corrects and produces a spurious positive relationship, because it interprets the total change variance as measurement error, when in reality it is a combination of measurement error and individual variance.

## 1.4: Conclusions

**When measurement error dominates, and individual variance is minimal**: Spurious negative relationship in observed change vs. initial; correctly removed by KP adjustment. [spurious negative relationship arises from regression to the mean caused by measurement error]. 

**When individual variance dominates, and measurement error is minimal**: No negative relationship in observed vs. change (correct); and spurious positive relationship arises from KP adjustment. [KP adjustment cannot distinguish individual variance and measurement error, considers all change variance as measurement error and attempts to adjust for regression to the mean; but regression to the mean is not actually happening when there's minimal measurement error]. 

**When individual variance and measurement error contribute equally**: Spurious negative relationship in observed change vs. initial [caused by regression to the mean from measurement error]; AND spurious positive relationship in KP-adjustment [caused by KP OVERestimating measurement error as total change variance]. 




# Scenario 2: True negative relationship between change and initial 

## 2.1: A true effect of initial value dominates variance in change

```{r s2.1_new, fig.width = 10, fig.height = 5.5}
# Scenario D: Mostly deterministic, with minimal v_ind and v_err (bÂ² = 0.98)
# Total variance = bÂ² + v_ind + 2*v_err = 0.98 + 0.01 + 0.01 = 1.00
v_ind <- 0.01
v_err <- 0.005
b     <- -sqrt(0.98)

# Run the simulation with the base plot templates (your plots1 list)
s2.1_results <- run_simulation_scenario(
  n = n, b = b, v_ind = v_ind, v_err = v_err, v_pop = v_pop)

# Plot it
annotated_panel_plot(s2.1_results$plots)
```

* A true relationship with minimal individual variance or measurement error is reflected accurately in the observed relationship **(G)**. However, Kelly-Price still interprets the variance in change as being due to measurement error, and attempts to correct for regression to the mean, yielding a much weaker relationship **(D and H)**.  

## 2.2: Equal true relationship, individual variance, and measurement error
```{r s2.2, fig.width = 10, fig.height = 5.5}
# Equal contribution from all sources
# Total variance = bÂ² + v_ind + 2*v_err = 1/3 + 1/3 + 1/3 = 1.00
b     <- -sqrt(1/3)
v_ind <- 1/3
v_err <- (1/3) / 2  # = 1/6

# Run the simulation with the base plot templates (your plots1 list)
s2.4_results <- run_simulation_scenario(
  n = n, b = b, v_ind = v_ind, v_err = v_err, v_pop = v_pop)

# Plot it
annotated_panel_plot(s2.4_results$plots)
```

* Shows same as above, with true relationship slightly overestimated in the observed change vs. initial **(G)**, and Kelly-Price adjustment **(H)** nearly masking the true relationship (though it is still there). What if the true effect is not as strong?

## 2.3: True relationship with measurement error dominating
```{r s2.3, fig.width = 10, fig.height = 5.5}
# Weaker true relationship with measurement error, minimal individual variance
# Total variance = bÂ² + v_ind + 2*v_err = 0.025 + 0.01 + 0.965 = 1.00
b     <- -sqrt(0.025)
v_ind <- 0.01
v_err <- (1 - 0.025 - v_ind) / 2  # = 0.4825

# Run the simulation with the base plot templates (your plots1 list)
s2.2_results <- run_simulation_scenario(
  n = n, b = b, v_ind = v_ind, v_err = v_err, v_pop = v_pop)

# Plot it
annotated_panel_plot(s2.2_results$plots)
```

* When measurement error dominates along with a true relationship, observed negative relationship is overestimated **(G)** due to RTM, and K-P adjustment nearly masks true relationship (**H**; still there but very weak).

## 2.4: True relationship with individual variance dominating
```{r s2.4, fig.width = 10, fig.height = 5.5}
# Weaker true relationship and individual variance, minimal measurement error
# Total variance = bÂ² + v_ind + 2*v_err = 0.025 + 0.965 + 0.01 = 1.00
b     <- -sqrt(0.025)
v_ind <- 0.965
v_err <- (1 - 0.025 - v_ind) / 2  # = 0.005

# Run the simulation with the base plot templates (your plots1 list)
s2.3_results <- run_simulation_scenario(
  n = n, b = b, v_ind = v_ind, v_err = v_err, v_pop = v_pop)

# Plot it
annotated_panel_plot(s2.3_results$plots)
```

* When individual variance dominates along with a true relationship and measurement error is minimal, the observed relationship (**G**) is accurate, but K-P produces spurious positive relationship **(H)**.

## 2.5: True relationship with equal individual variance and meaurement error
```{r s2.5, fig.width = 10, fig.height = 5.5}
# Mixed â€” moderate contributions from all sources
# Total variance = bÂ² + v_ind + 2*v_err = 0.025 + 0.4875 + 0.4875 = 1.00
b     <- -sqrt(0.025)
v_ind <- 0.4875
v_err <- (1 - 0.025 - v_ind) / 2  # = 0.24375

# Run the simulation with the base plot templates (your plots1 list)
s2.4_results <- run_simulation_scenario(
  n = n, b = b, v_ind = v_ind, v_err = v_err, v_pop = v_pop)

# Plot it
annotated_panel_plot(s2.4_results$plots)
```

* When both individual variance and measurement error contribute equally, observed relationship is overly negative (due to measurement error/RTM) and K-P adjustment is spuriously positive (due to overcorrection because assumes that individual variance is measurement error). 

# Dataset 1: *P. damicornis* symbiont changes

from Cunning and Baker 2013

### Import and tidy data
```{r pdam_import_tidy}
# Import and tidy Pdam data
pdam0 <- read_csv("PdamRbleaching.csv")
pdam <- pdam0 %>%
  mutate(log_init_raw = log(juntotal),
         log_final_raw = log(augtotal),
         log_change = log_final_raw - log_init_raw)


# Remove symbiont effect
## Calculate grand mean
grand_mean_init <- mean(pdam$log_init_raw)
grand_mean_final <- mean(pdam$log_final_raw)
## Add sym-group residuals to grand mean
pdam <- pdam %>%
  group_by(sym) %>%
  mutate(group_resid_init = log_init_raw - mean(log_init_raw),
         group_resid_final = log_final_raw - mean(log_final_raw)) %>%
  ungroup() %>%
  mutate(log_init_obs = grand_mean_init + group_resid_init,
         log_final_obs = grand_mean_final + group_resid_final,
         log_change_obs = log_final_obs - log_init_obs) %>%
  select(colony, log_init_obs, log_final_obs, log_change_obs)

# Get pdam variances
# Total variance in initial observed trait values
pdam_var_x_abs <- var(pdam$log_init_obs, na.rm = TRUE)

# Measurement error variance (from qPCR technical replicates; see pdam_variance.R)
pdam_v_err_abs <- 0.0231325     # on trait scale (logSH)
```

### Observed and adjusted relationships in *P. damicornis* study
```{r}
# --- 2) Observed slope + CI ---
pdam_fit_obs <- lm(log_change_obs ~ log_init_obs, data = pdam)
pdam_b_obs   <- coef(pdam_fit_obs)[2]
pdam_b_obs_confint <- confint(pdam_fit_obs)[2, ]

# --- 3) Kellyâ€“Price (point + bootstrap CI) ---
pdam_kp_full <- rttm.adj(pdam$log_init_obs, pdam$log_final_obs)
pdam_b_kp_adj <- coef(lm(pdam_kp_full$adj.growth ~ pdam$log_init_obs))[2]

set.seed(100)
pdam_kp_boot <- boot::boot(
  data = pdam,
  statistic = function(d, i) {
    di <- d[i, , drop = FALSE]
    kp <- rttm.adj(di$log_init_obs, di$log_final_obs)
    coef(lm(kp$adj.growth ~ di$log_init_obs))[2]
  },
  R = 2000
)
pdam_b_kp_adj_confint_boot <- stats::quantile(pdam_kp_boot$t, c(0.025, 0.975), na.rm = TRUE)

# --- 4) Blomqvist (point + bootstrap CI) ---
pdam_k <- pdam_v_err_abs / pdam_var_x_abs
pdam_b_blomq_adj <- blomqvist_b(pdam_b_obs, pdam_k)

# pseudo-exact yâ€™ (used only for plotting panel, not for CI)
pdam <- pdam %>%
  dplyr::mutate(
    log_change_kp_adj    = pdam_kp_full$adj.growth,
    log_change_blomq_adj = blomqvist_pseudo_exact(log_init_obs, log_change_obs, pdam_b_blomq_adj)
  )

set.seed(200)
pdam_blomqvist_boot <- boot::boot(
  data = pdam,
  statistic = function(d, i) {
    di <- d[i, , drop = FALSE]
    b_i <- coef(lm(log_change_obs ~ log_init_obs, data = di))[2]
    blomqvist_b(b_i, pdam_k)
  },
  R = 2000
)
pdam_b_blomq_adj_confint_boot <- stats::quantile(pdam_blomqvist_boot$t, c(0.025, 0.975), na.rm = TRUE)

# --- 5) Slopes tibble ---
pdam_slopes_tbl <- tibble::tibble(
  method   = factor(c("Obs", "K-P", "Blomq"), levels = c("Obs", "K-P", "Blomq")),
  estimate = c(pdam_b_obs, pdam_b_kp_adj, pdam_b_blomq_adj),
  ci_lower = c(pdam_b_obs_confint[1], pdam_b_kp_adj_confint_boot[1], pdam_b_blomq_adj_confint_boot[1]),
  ci_upper = c(pdam_b_obs_confint[2], pdam_b_kp_adj_confint_boot[2], pdam_b_blomq_adj_confint_boot[2])
)

# --- 6) Axis ranges for plots ---
# ## Compute new plot limit values
pdam_trait <- range(c(pdam$log_init_obs, pdam$log_final_obs), na.rm = TRUE)
pdam_change <- range(pdam$log_change_obs, na.rm = TRUE)

pdam_chiolero_plot <- chiolero_plot(pdam, log_init_obs, log_final_obs) + ggtitle("Observed")
pdam_initfinal_plot <- ggplot(pdam, aes(x = log_init_obs, y = log_final_obs)) +
    geom_point(alpha = 0.2) +
    geom_abline(slope = 1, intercept = 0, linetype = "dashed") +
    ggpubr::stat_cor(label.x = -Inf, label.y = Inf, hjust = -0.1, vjust = 1.5) +
    geom_smooth(method = "lm", se = FALSE) +
    coord_fixed(xlim = c(pdam_trait[1], pdam_trait[2]), ylim = c(pdam_trait[1], pdam_trait[2])) +
    ggtitle("Observed")
pdam_initchange_plot <- ggplot(pdam, aes(x = log_init_obs, y = log_change_obs)) +
    geom_point(alpha = 0.2) +
    geom_hline(yintercept = 0, linetype = "dashed") +
    ggpubr::stat_cor(label.x = -Inf, label.y = Inf, hjust = -0.1, vjust = 1.5) +
    geom_smooth(method = "lm", se = FALSE) +
    coord_fixed(xlim = c(pdam_trait[1], pdam_trait[2]), ylim = c(pdam_change[1], pdam_change[2])) +
    ggtitle("Observed")
pdam_kp_plot <- ggplot(pdam, aes(x = log_init_obs, y = log_change_kp_adj)) +
    geom_point(alpha = 0.2) +
    geom_hline(yintercept = 0, linetype = "dashed") +
    ggpubr::stat_cor(label.x = -Inf, label.y = Inf, hjust = -0.1, vjust = 1.5) +
    geom_smooth(method = "lm", se = FALSE) +
    coord_fixed(xlim = c(pdam_trait[1], pdam_trait[2]), ylim = c(pdam_change[1], pdam_change[2])) +
    ggtitle("Kelly-Price")
pdam_blomq_plot <- ggplot(pdam, aes(x = log_init_obs, y = log_change_blomq_adj)) +
    geom_point(alpha = 0.2) +
    geom_hline(yintercept = 0, linetype = "dashed") +
    ggpubr::stat_cor(label.x = -Inf, label.y = Inf, hjust = -0.1, vjust = 1.5) +
    geom_smooth(method = "lm", se = FALSE) +
    coord_fixed(xlim = c(pdam_trait[1], pdam_trait[2]), ylim = c(pdam_change[1], pdam_change[2])) +
    ggtitle("Blomqvist")
pdam_slopes_plot <- ggplot(pdam_slopes_tbl, aes(x = method, y = estimate)) +
  geom_point() +
  geom_errorbar(aes(ymin = ci_lower, ymax = ci_upper))

cowplot::plot_grid(pdam_chiolero_plot, pdam_initfinal_plot, NULL, NULL, pdam_initchange_plot, 
                   pdam_kp_plot, pdam_blomq_plot, pdam_slopes_plot, nrow = 2, align = "hv")
```




# Dataset 2: *A. cervicornis* growth

from Million et al.

### Import and tidy data
```{r mill_import_tidy}
# Import data
mill <- read_csv("subMil1.csv")       # Generated in Carly's script

# Tidy and filter data
mill <- mill %>%
  filter(size > 0) %>%  # there are four rows where size = 0, filter out because will generate Inf
  filter(InitialSize > 1) %>%    # filter out very small frags (noisy) (30 observations with InitialSize < 1)
  mutate(log_init_obs = log(InitialSize),
         log_final_obs = log(size),
         log_change_obs = log_final_obs - log_init_obs)

# Variance in initial observed trait values
mill_var_x_abs <- var(mill$log_init_obs, na.rm = TRUE)
# Measurement error variance (from qPCR technical replicates; see mill_variance.R)
mill_v_err_abs <- 0.01301945     # on trait scale (logTLE; see "RegressionToMean_forGithub.R")
```

### Observed and adjusted relationships in *A. cervicornis* study
```{r}
# --- 2) Observed slope + CI ---
mill_fit_obs <- lm(log_change_obs ~ log_init_obs, data = mill)
mill_b_obs   <- coef(mill_fit_obs)[2]
mill_b_obs_confint <- confint(mill_fit_obs)[2, ]

# --- 3) Kellyâ€“Price (point + bootstrap CI) ---
mill_kp_full <- rttm.adj(mill$log_init_obs, mill$log_final_obs)
mill_b_kp_adj <- coef(lm(mill_kp_full$adj.growth ~ mill$log_init_obs))[2]

set.seed(100)
mill_kp_boot <- boot::boot(
  data = mill,
  statistic = function(d, i) {
    di <- d[i, , drop = FALSE]
    kp <- rttm.adj(di$log_init_obs, di$log_final_obs)
    coef(lm(kp$adj.growth ~ di$log_init_obs))[2]
  },
  R = 2000
)
mill_b_kp_adj_confint_boot <- stats::quantile(mill_kp_boot$t, c(0.025, 0.975), na.rm = TRUE)

# --- 4) Blomqvist (point + bootstrap CI) ---
mill_k <- mill_v_err_abs / mill_var_x_abs
mill_b_blomq_adj <- blomqvist_b(mill_b_obs, mill_k)

# pseudo-exact yâ€™ (used only for plotting panel, not for CI)
mill <- mill %>%
  dplyr::mutate(
    log_change_kp_adj    = mill_kp_full$adj.growth,
    log_change_blomq_adj = blomqvist_pseudo_exact(log_init_obs, log_change_obs, mill_b_blomq_adj)
  )

set.seed(200)
mill_blomqvist_boot <- boot::boot(
  data = mill,
  statistic = function(d, i) {
    di <- d[i, , drop = FALSE]
    b_i <- coef(lm(log_change_obs ~ log_init_obs, data = di))[2]
    blomqvist_b(b_i, mill_k)
  },
  R = 2000
)
mill_b_blomq_adj_confint_boot <- stats::quantile(mill_blomqvist_boot$t, c(0.025, 0.975), na.rm = TRUE)

# --- 5) Slopes tibble ---
mill_slopes_tbl <- tibble::tibble(
  method   = factor(c("Obs", "K-P", "Blomq"), levels = c("Obs", "K-P", "Blomq")),
  estimate = c(mill_b_obs, mill_b_kp_adj, mill_b_blomq_adj),
  ci_lower = c(mill_b_obs_confint[1], mill_b_kp_adj_confint_boot[1], mill_b_blomq_adj_confint_boot[1]),
  ci_upper = c(mill_b_obs_confint[2], mill_b_kp_adj_confint_boot[2], mill_b_blomq_adj_confint_boot[2])
)

# --- 6) Axis ranges for plots ---
# ## Compute new plot limit values
mill_trait <- range(c(mill$log_init_obs, mill$log_final_obs), na.rm = TRUE)
mill_change <- range(mill$log_change_obs, na.rm = TRUE)

mill_chiolero_plot <- chiolero_plot(mill, log_init_obs, log_final_obs) + ggtitle("Observed")
mill_initfinal_plot <- ggplot(mill, aes(x = log_init_obs, y = log_final_obs)) +
    geom_point(alpha = 0.2) +
    geom_abline(slope = 1, intercept = 0, linetype = "dashed") +
    ggpubr::stat_cor(label.x = -Inf, label.y = Inf, hjust = -0.1, vjust = 1.5) +
    geom_smooth(method = "lm", se = FALSE) +
    coord_fixed(xlim = c(mill_trait[1], mill_trait[2]), ylim = c(mill_trait[1], mill_trait[2])) +
    ggtitle("Observed")
mill_initchange_plot <- ggplot(mill, aes(x = log_init_obs, y = log_change_obs)) +
    geom_point(alpha = 0.2) +
    geom_hline(yintercept = 0, linetype = "dashed") +
    ggpubr::stat_cor(label.x = -Inf, label.y = Inf, hjust = -0.1, vjust = 1.5) +
    geom_smooth(method = "lm", se = FALSE) +
    coord_fixed(xlim = c(mill_trait[1], mill_trait[2]), ylim = c(mill_change[1], mill_change[2])) +
    ggtitle("Observed")
mill_kp_plot <- ggplot(mill, aes(x = log_init_obs, y = log_change_kp_adj)) +
    geom_point(alpha = 0.2) +
    geom_hline(yintercept = 0, linetype = "dashed") +
    ggpubr::stat_cor(label.x = -Inf, label.y = Inf, hjust = -0.1, vjust = 1.5) +
    geom_smooth(method = "lm", se = FALSE) +
    coord_fixed(xlim = c(mill_trait[1], mill_trait[2]), ylim = c(mill_change[1], mill_change[2])) +
    ggtitle("Kelly-Price")
mill_blomq_plot <- ggplot(mill, aes(x = log_init_obs, y = log_change_blomq_adj)) +
    geom_point(alpha = 0.2) +
    geom_hline(yintercept = 0, linetype = "dashed") +
    ggpubr::stat_cor(label.x = -Inf, label.y = Inf, hjust = -0.1, vjust = 1.5) +
    geom_smooth(method = "lm", se = FALSE) +
    coord_fixed(xlim = c(mill_trait[1], mill_trait[2]), ylim = c(mill_change[1], mill_change[2])) +
    ggtitle("Blomqvist")
mill_slopes_plot <- ggplot(mill_slopes_tbl, aes(x = method, y = estimate)) +
  geom_point() +
  geom_errorbar(aes(ymin = ci_lower, ymax = ci_upper))

cowplot::plot_grid(mill_chiolero_plot, mill_initfinal_plot, NULL, NULL, mill_initchange_plot, 
                   mill_kp_plot, mill_blomq_plot, mill_slopes_plot, nrow = 2, align = "hv")
```




# Validation: Synthetic data

```{r evaluate_methods_and_1_parset_similar_to_pdam}
# Create a function that will create synthetic datasets with known true slope and variance structure, and then test the ability of the four estimation methods (observed slope, kelly-price adjusted, blomqvist adjusted, and simulation-inferred) to recover the true slope

evaluate_methods <- function(v_pop, k, v_ind_rel, b_true, reps, n, v_err, v_ind, seed) {
  replicate_results <- purrr::map_dfr(seq_len(reps), function(i) {
    set.seed(seed + i)  # Unique seed per replicate

    # Simulate data
    log_init_true <- rnorm(n, 0, sqrt(v_pop))
    log_change_true <- b_true * log_init_true + rnorm(n, 0, sqrt(v_ind))
    log_final_true <- log_init_true + log_change_true
    e_init <- rnorm(n, 0, sqrt(v_err))
    e_final <- rnorm(n, 0, sqrt(v_err))

    syn_df <- tibble(
      log_init_obs = log_init_true + e_init,
      log_final_obs = log_final_true + e_final,
      log_change_obs = log_final_obs - log_init_obs
    ) %>%
      mutate(log_change_kp_adj = rttm.adj(log_init_obs, log_final_obs)[, 3])

    # Observed slope
    obs_fit <- lm(log_change_obs ~ log_init_obs, data = syn_df)
    b_obs <- coef(obs_fit)[[2]]
    
    # Kelly-Price adjusted slope
    kp_fit <- lm(log_change_kp_adj ~ log_init_obs, data = syn_df)
    b_kp <- coef(kp_fit)[[2]]
    
    # Blomqvist adjusted slope
    k_blomq <- v_err / (v_pop + v_err)
    b_blomq <- (b_obs + k_blomq) / (1 - k_blomq)
    
    # Results of all estimates
    tibble(
      b_obs = b_obs,
      b_blomq = b_blomq,
      b_kp = b_kp,
      b_true = b_true
    )
  })

  # Summarize results across replicates using `across()`
  summarised <- replicate_results %>%
    summarise(across(
      .cols = c(b_obs, b_blomq, b_kp),
      .fns = list(
        mean = ~ mean(.x, na.rm = TRUE),
        sd   = ~ sd(.x, na.rm = TRUE)
      ),
      .names = "{.col}_{.fn}"
    )) %>%
    mutate(
      b_true = b_true,
      v_ind = v_ind,
      v_err = v_err,
      k = k,
      v_ind_rel = v_ind_rel
    )

  return(summarised)
}


# # Test the ability of the four estimation methods to recover the true slope for synthetic datasets with the same size (n=42) and variance structure as the Pdam dataset
# 
# test_syn_eval <- evaluate_methods(
#   b_true = -0.855,
#   v_ind = var(pdam$log_change_obs) - ((-0.855)^2 * (var(pdam$log_init_obs) - pdam_v_err_abs)) - (2 * pdam_v_err_abs),
#   v_err = pdam_v_err_abs,   # pdam_v_err_abs
#   v_pop = var(pdam$log_init_obs) - pdam_v_err_abs,   # pdam_v_pop_abs
#   reps = 1000,      # number of replicate synthetic datasets of size n that will be simulated and tested
#   n = 42,         # size of synthetic dataset to be tested
#   seed = 123     # base seed that will have +i for all synthetic dataset reps 1:i
# )
# 
# # Plot outcomes
# # Tidy the summary output
# plot_df <- test_syn_eval %>%
#   pivot_longer(
#     cols = c(b_obs_mean, b_blomq_mean, b_kp_mean,
#              b_obs_sd, b_blomq_sd, b_kp_sd),
#     names_to = c("method", ".value"),
#     names_pattern = "b_(.*)_(mean|sd)"
#   ) %>%
#   mutate(
#     method = recode(method,
#       "obs"   = "Observed",
#       "blomq" = "Blomqvist Corrected",
#       "kp"    = "Kelly-Price Adjusted"
#     ),
#     method = factor(method, levels = c(
#       "Observed", "Blomqvist Corrected", "Kelly-Price Adjusted"
#     ))
#   )
# 
# # Plot the mean estimate Â±1 SD for each method
# ggplot(plot_df, aes(x = method, y = mean)) +
#   geom_point(size = 3) +
#   geom_errorbar(aes(ymin = mean - sd, ymax = mean + sd), width = 0.2) +
#   geom_hline(yintercept = test_syn_eval$b_true, linetype = "dashed", color = "blue") +
#   annotate("text", x = 4.5, y = test_syn_eval$b_true, label = "True slope", hjust = 1,
#            vjust = -0.5, color = "blue", fontface = "italic") +
#   labs(
#     title = "Performance of Slope Estimators on Synthetic Dataset",
#     y = "Estimated slope (mean Â± SD)",
#     x = NULL
#   ) +
#   coord_flip(clip = "off") +
#   theme_minimal(base_size = 14)

```

So for datasets like the Pdam dataset, the simulation-inferred estimate is the best estimate of the true underlying relationship. 

What about for more diverse datasets with varying true relationships, v_err, and v_ind? Evaluate the methods over a whole parameter grid.

```{r evaluate_methods_parameter_grid}

# Create parameter grid for testing all slope correction methods
param_grid <- expand.grid(
  v_pop = 1,                                        # always standardize to v_pop = 1
  k = c(0.01, 1/6, 0.5),                           # measurement imprecision (v_err / (v_pop + v_err))
  v_ind_rel = c(2, 0.5, 0.01),                      # range of true v_ind in synthetic datasets
  b_true = c(-0.9, -0.5, -0.2, 0, 0.2, 0.5, 0.9),   # range of true slopes in synthetic datasets
  reps = 1000,                                       # n replicate synthetic datasets for each parameter set
  n = 200                    # size of each synthetic dataset (should represent a 'typical' scientific study)
) 

# Calculate v_err from given measurement precision level
param_grid <- param_grid %>%
  mutate(v_err = v_pop * k / (1 - k),
         v_ind = v_pop * v_ind_rel)

# Each parameter set gets a different seed to use in generating simulated dataset
set.seed(999)  # for reproducibility of seeds in parameter grid
param_grid <- param_grid %>%
  dplyr::mutate(seed = sample.int(1e6, size = dplyr::n(), replace = FALSE))


# Apply function to each grid row (in parallel)
plan(multisession)
results_grid <- future_pmap_dfr(
  param_grid,
  evaluate_methods,
  .options = furrr_options(seed = TRUE, packages = c("boot", "broom"))
)
saveRDS(results_grid, file = "results_grid.rds")
results_grid <- readRDS("results_grid.rds")

# Pivot to long format for plotting
results_long <- results_grid %>%
  pivot_longer(
    cols = c(b_obs_mean, b_kp_mean, b_blomq_mean),
    names_to = "method",
    values_to = "estimate"
  ) %>%
  mutate(
    sd = case_when(
      method == "b_obs_mean"    ~ b_obs_sd,
      method == "b_kp_mean"     ~ b_kp_sd,
      method == "b_blomq_mean"  ~ b_blomq_sd
    ),
    error = estimate - b_true
  )

# Facet labeller 
facet_labeller <- function(labels) {
  to_num <- function(x) suppressWarnings(as.numeric(as.character(x)))
  fmt    <- function(x) formatC(x, format = "f", digits = 2)

  if ("k" %in% names(labels)) {
    k <- to_num(labels$k)
    k_cat <- ifelse(k <= 0.05, "Very low",
             ifelse(k <= 0.25, "Moderate",
             ifelse(k <= 0.50, "High", "Very high")))
    labels$k <- paste0(k_cat, " (", fmt(k/(1-k)), ")")
  }

  if ("v_ind" %in% names(labels)) {
    v <- to_num(labels$v_ind)
    v_cat <- ifelse(v <= 0.05, "Very low",
             ifelse(v <= 0.50, "Moderate",
             ifelse(v <= 2.00, "High", "Extreme")))
    labels$v_ind <- paste0(v_cat, " (", fmt(v / v_pop), ")")
  }

  labels
}


ggplot(results_long, aes(
  x = b_true,
  y = pmax(pmin(error, 0.4), -0.4),
  color = method,
  group = method,
  shape = factor(ifelse(error > 0.4, 2, ifelse(error < -0.4, 6, 16)))
)) +
  geom_line(linewidth = 0.5, alpha = 0.6) +
  geom_errorbar(aes(ymin = error - sd, ymax = error + sd), width = 0.05, alpha = 0.6) +
  geom_point(size = 2, alpha = 0.6) +
  scale_shape_manual(values = c("2" = 2, "6" = 6, "16" = 16), guide = "none") +
  scale_color_discrete(
    labels = c(
      b_obs_mean = "Observed",
      b_blomq_mean = "Blomqvist",
      b_kp_mean = "Kelly-Price"
    )
  ) +
  geom_hline(yintercept = 0, linetype = "dashed") +
  facet_grid(v_ind ~ k, labeller = facet_labeller) +
  coord_cartesian(ylim = c(-0.4, 0.4)) +
  labs(
    x = "True slope",
    y = "Slope error (estimate - true)",
    color = "Method"
  ) +
  theme_minimal(base_size = 12) +
  scale_x_continuous(
    sec.axis = sec_axis(~ .,
                        name = expression(Initial~measurement~error~~(sigma[e]^2/sigma[X]^2)),
                        breaks = NULL, labels = NULL
    )
  ) +
  scale_y_continuous(
    sec.axis = sec_axis(~ .,
                        name = expression(Change~heterogeneity~~(sigma[Delta[ind]]^2/sigma[X]^2)),
                        breaks = NULL, labels = NULL)
  )



ggplot(results_long, aes(x = b_true, y = b_true + error, color = method, shape = method)) +
  #geom_point() +
  geom_ribbon(aes(ymin = b_true + error - sd, ymax = b_true + error + sd, fill = method), alpha = 0.5, color = NA) +
  #geom_line(aes(group = method)) +
  geom_abline(slope = 1, intercept = 0, lty = 2) +
  facet_grid(v_ind ~ v_err, labeller = label_both)
```

* Observed relationships are highly susceptible to spurious negative overestimation, which worsens as measurement error increases.
* Kelly-Price adjustments are highly susceptible to spurious positive overestimation, especially when the true relationship is negative, and as v_ind increases. 
* Blomqvist adjustments **consistently recover the accurate true relationship across all datasets.**


```{r, include = F, eval = F}
# Run the simulation with the base plot templates (your plots1 list)
b <- 0.5
n <- 1000
v_ind <- 2
v_err <- 0.4
v_pop <- 1

tsym <- run_simulation_scenario(
  n = n, b = b, v_ind = v_ind, v_err = v_err, v_pop = v_pop)

annotated_panel_plot(tsym$plots)

tsym$data


dat <- tsym$data
x <- dat$log_init_obs
y <- dat$log_change_obs

# 1) OLS (y ~ x)
fit_lm <- lm(y ~ x)
b_ols  <- unname(coef(fit_lm)[2])
a_ols  <- unname(coef(fit_lm)[1])

# 2) Orthogonal regression (major axis / total least squares)
orth_slope <- function(x, y) {
  Sxx <- var(x, na.rm = TRUE)
  Syy <- var(y, na.rm = TRUE)
  Sxy <- cov(x, y, use = "complete.obs")
  if (is.na(Sxy) || abs(Sxy) < .Machine$double.eps) return(NA_real_)
  sign_sxy <- ifelse(Sxy >= 0, 1, -1)
  (Syy - Sxx + sign_sxy * sqrt((Syy - Sxx)^2 + 4 * Sxy^2)) / (2 * Sxy)
}
b_orth <- orth_slope(x, y)
a_orth <- mean(y, na.rm = TRUE) - b_orth * mean(x, na.rm = TRUE)

# Nice annotation positions
rx <- range(x, na.rm = TRUE); ry <- range(y, na.rm = TRUE)
x_anno <- rx[1] + 0.03 * diff(rx)
y_anno <- ry[2] - 0.05 * diff(ry)

p_lm <- ggplot(dat, aes(x, y)) +
  geom_point(alpha = 0.3) +
  geom_abline(slope = b_ols, intercept = a_ols) +
  annotate("text", x = x_anno, y = y_anno,
           label = sprintf("OLS slope = %.3f", b_ols), hjust = 0, vjust = 1) +
  labs(title = "OLS: y ~ x", x = "log_init_obs", y = "log_change_obs") +
  theme_classic()

p_orth <- ggplot(dat, aes(x, y)) +
  geom_point(alpha = 0.3) +
  geom_abline(slope = b_orth, intercept = a_orth) +
  annotate("text", x = x_anno, y = y_anno,
           label = sprintf("Orthogonal slope = %.3f", b_orth), hjust = 0, vjust = 1) +
  labs(title = "Orthogonal (Major Axis)", x = "log_init_obs", y = "log_change_obs") +
  theme_classic()

cowplot::plot_grid(p_lm, p_orth, nrow = 1)

```


```{r}
# relationship between observed slope, blomqvist's k, and blomqvist's slope
library(ggplot2)

b_true <- seq(-1, 1, 0.01) # true slope of -1 means all final trait values are zero, cannot have a lower slope than this
k_vals <- c(0, 0.1, 0.3, 0.5, 0.7)

df <- expand.grid(b_true = b_true, k = k_vals)
df$b_obs <- (1 - df$k) * df$b_true - df$k

ggplot(df, aes(x = b_true, y = b_obs, color = factor(k))) +
  geom_line(size = 1) +
  geom_abline(slope = 1, intercept = 0, linetype = 3) +
  labs(x = "True slope of change vs. initial", 
       y = "Observed slope of change vs. initial", 
       color = "k (fraction of observed\ntrait variance due to\nmeasurement error)") +
  theme_bw()

```
